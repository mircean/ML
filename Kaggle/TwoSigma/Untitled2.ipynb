{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from scipy.special import expit\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "#from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from xgboost.core import DMatrix\n",
    "from xgboost.training import train, cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data 2017-03-09 23:05:11.859354\n",
      "(49352, 15)\n",
      "(74659, 14)\n",
      "load data done 2017-03-09 23:05:16.255029\n"
     ]
    }
   ],
   "source": [
    "train_file = 'Dataset\\\\train.json'\n",
    "test_file = 'Dataset\\\\test.json'\n",
    "print('load data', datetime.datetime.now())\n",
    "df_train = pd.read_json(train_file)\n",
    "df_test = pd.read_json(test_file)\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "print('load data done', datetime.datetime.now())\n",
    "\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "y_train = np.array(df_train['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "df_train = df_train.drop(['interest_level'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_engineering(df_train, df_test, y_train):\n",
    "    print('feature engineering', datetime.datetime.now())\n",
    "\n",
    "    #for some reason listing_id improves the score\n",
    "\n",
    "    #df_train.index = df_train['listing_id']\n",
    "    #df_train = df_train.drop(['listing_id'], axis=1)\n",
    "\n",
    "    #df_test.index = df_test['listing_id']\n",
    "    #df_test = df_test.drop(['listing_id'], axis=1)\n",
    "\n",
    "    #ignore_Index because use sort_index later.\n",
    "    df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True)\n",
    "\n",
    "    ###\n",
    "    ###date feature\n",
    "    ###\n",
    "    df_all['created'] = pd.to_datetime(df_all['created'])\n",
    "    #year - all 2016\n",
    "    #df['created_year'] = df['created'].dt.year\n",
    "    df_all['created_month'] = df_all['created'].dt.month\n",
    "    df_all['created_day'] = df_all['created'].dt.day\n",
    "    df_all['created_day_of_year'] = df_all['created'].dt.strftime('%j').astype(int)\n",
    "    df_all['created_hour'] = df_all['created'].dt.hour\n",
    "    df_all['created_weekday'] = df_all['created'].dt.weekday\n",
    "    df_all = df_all.drop(['created'], axis=1)\n",
    "\n",
    "    '''\n",
    "    #create_weekday categorical\n",
    "    ohe = OneHotEncoder(sparse=False)\n",
    "    df_all_ohe = ohe.fit_transform(df_all.created_weekday.reshape(-1, 1)) \t\n",
    "    for i in range(df_all_ohe.shape[1]):\n",
    "        df_all['ohe' + str(i)] = df_all_ohe[:, i]\n",
    "    df_all = df_all.drop(['created_weekday'], axis=1)\n",
    "    '''\n",
    "    ###\n",
    "    ### numeric features\n",
    "    ###\n",
    "    #adjust incorrect x/y\n",
    "    x_mean = df_all.latitude.mean()\n",
    "    y_mean = df_all.longitude.mean()\n",
    "\n",
    "    df_all.loc[df_all.latitude < x_mean - 5, 'latitude'] = x_mean - 5\n",
    "    df_all.loc[df_all.latitude > x_mean + 5, 'latitude'] = x_mean + 5\n",
    "    df_all.loc[df_all.longitude < y_mean - 5, 'longitude'] = y_mean - 5\n",
    "    df_all.loc[df_all.longitude > y_mean + 5, 'longitude'] = y_mean + 5\n",
    "\n",
    "    '''\n",
    "    #adjust incorrect x/y by percentile\n",
    "    percentile = 0.1\n",
    "    llimit = np.percentile(df_all.latitude.values, percentile)\n",
    "    ulimit = np.percentile(df_all.latitude.values, 100 - percentile)\n",
    "    df_all.loc[df_all['latitude']<llimit, 'latitude'] = llimit\n",
    "    df_all.loc[df_all['latitude']>ulimit, 'latitude'] = ulimit\n",
    "    llimit = np.percentile(df_all.longitude.values, percentile)\n",
    "    ulimit = np.percentile(df_all.longitude.values, 100 - percentile)\n",
    "    df_all.loc[df_all['longitude']<llimit, 'longitude'] = llimit\n",
    "    df_all.loc[df_all['longitude']>ulimit, 'longitude'] = ulimit\n",
    "    '''\n",
    "\n",
    "    #log x/y\n",
    "    df_all['logx'] = np.log(df_all['latitude'])\n",
    "    df_all['logy'] = np.log(df_all['longitude'] + 100)\n",
    "\n",
    "    #radius\n",
    "    df_all['radius'] = np.log((df_all.latitude - x_mean)*(df_all.latitude - x_mean) + (df_all.longitude - y_mean)*(df_all.longitude - y_mean))\n",
    "\n",
    "    #price\n",
    "    #df_all.loc[df_all['price'] > 100000, 'price'] = 100000\n",
    "\n",
    "    #log price\n",
    "    #df_all['logprice'] = np.log(df_all.price)\n",
    "\n",
    "    df_all[\"price_per_bed\"] = df_all[\"price\"]/df_all[\"bedrooms\"] \n",
    "    df_all[\"room_dif\"] = df_all[\"bedrooms\"] - df_all[\"bathrooms\"] \n",
    "    df_all[\"room_sum\"] = df_all[\"bedrooms\"] + df_all[\"bathrooms\"] \n",
    "    df_all[\"price_per_room\"] = df_all[\"price\"]/df_all[\"room_sum\"]\n",
    "\n",
    "    #replace np.inf\n",
    "    df_all.loc[df_all.price_per_bed == np.inf, 'price_per_bed'] = 10000000\n",
    "    df_all.loc[df_all.price_per_room == np.inf, 'price_per_room'] = 10000000\n",
    "\n",
    "    df_all[\"photos_count\"] = df_all[\"photos\"].apply(len)\n",
    "    df_all = df_all.drop(['photos'], axis=1)\n",
    "    \n",
    "    ###\n",
    "    ###zones\n",
    "    ###\n",
    "    n_zones = 140\n",
    "    x_min = df_all.logx.mean() - 0.004\n",
    "    x_max = df_all.logx.mean() + 0.003\n",
    "    y_min = df_all.logy.mean() - 0.003\n",
    "    y_max = df_all.logy.mean() + 0.003\n",
    "\n",
    "    df_all2 = df_all[(df_all.logx >= x_min) & (df_all.logx <= x_max) & (df_all.logy >= y_min) & (df_all.logy <= y_max)]\n",
    "    kmeans = KMeans(n_clusters=n_zones, random_state=0).fit(df_all2[['logx', 'logy']])\n",
    "\n",
    "    print('zones', df_all.shape)\n",
    "\n",
    "    for i in range(n_zones):\n",
    "        df_all['zone' + str(i)] = 0\n",
    "        df_all.loc[df_all2.logx[kmeans.labels_ == i].index, 'zone' + str(i)] = 1\n",
    "\n",
    "    print('zones', df_all.shape)\n",
    "\n",
    "    ###\n",
    "    ###description\n",
    "    ###\n",
    "    df_all['description'] = df_all['description'].apply(lambda x: cleaning_text(x))\n",
    "    df_all[\"description_words_count\"] = df_all[\"description\"].apply(lambda x: 0 if len(x) == 0 else len(x.split(\" \")))\n",
    "    df_all['description_uppercase'] = df_all['description'].apply(lambda x: 1 if len(re.findall(r'[A-Z]', x))/(len(x) + 1) > 0.5 else 0)\n",
    "    df_all['description'] = df_all['description'].apply(lambda x: x.lower())\n",
    "\n",
    "    '''\n",
    "    n_features2 = 100\n",
    "    tfidf2 = CountVectorizer(stop_words='english', max_features=n_features2)\n",
    "    tr_sparse2 = tfidf2.fit_transform(df_all[:df_train.shape[0]]['description'])\n",
    "    te_sparse2 = tfidf2.transform(df_all[df_train.shape[0]:]['description'])\n",
    "    '''\n",
    "    df_all = df_all.drop(['description'], axis=1)\n",
    "\n",
    "    ###\n",
    "    ### features\n",
    "    ###\n",
    "    df_all[\"features_count\"] = df_all[\"features\"].apply(len)\n",
    "\n",
    "    n_features = 2000\n",
    "    df_all['features'] = df_all['features'].apply(lambda x: cleaning_list(x))\n",
    "    df_all['features'] = df_all['features'].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "    tfidf = CountVectorizer(stop_words='english', max_features=n_features)\n",
    "    tr_sparse = tfidf.fit_transform(df_all[:df_train.shape[0]]['features'])\n",
    "    te_sparse = tfidf.transform(df_all[df_train.shape[0]:]['features'])\n",
    "\n",
    "    df_all = df_all.drop(['features'], axis=1)\n",
    "\n",
    "    ###\n",
    "    ###display and street address\n",
    "    ###\n",
    "    df_all['display_address'] = df_all.display_address.str.replace('Avenue', '')\n",
    "    df_all['display_address'] = df_all.display_address.str.replace(' Ave', '')\n",
    "    df_all['display_address'] = df_all.display_address.str.replace('Street', '')\n",
    "    df_all['display_address'] = df_all.display_address.str.replace('St.', '')\n",
    "    df_all['display_address'] = df_all.display_address.str.replace(' St', '')\n",
    "    df_all['display_address'] = df_all.display_address.str.rstrip()\n",
    "\n",
    "    df_all['street_address'] = df_all.street_address.str.replace('Avenue', '')\n",
    "    df_all['street_address'] = df_all.street_address.str.replace(' Ave', '')\n",
    "    df_all['street_address'] = df_all.street_address.str.replace('Street', '')\n",
    "    df_all['street_address'] = df_all.street_address.str.replace('St.', '')\n",
    "    df_all['street_address'] = df_all.street_address.str.replace(' St', '')\n",
    "    df_all['street_address'] = df_all.street_address.str.rstrip()\n",
    "\n",
    "    #keep only the first int from street_address - not a good idea, just the number without street is useless\n",
    "    #df_all['street_address'] = df_all.street_address.apply(lambda x: x.split(\" \")[0])\n",
    "\n",
    "    ###\n",
    "    ###categorical features\n",
    "    ###\n",
    "    #cannot make them ohe - too many distinct values\n",
    "    ohe_features = ['building_id', 'display_address', 'manager_id', 'street_address'] \n",
    "    for f in ohe_features: \n",
    "        le = LabelEncoder() \n",
    "        df_all[f] = le.fit_transform(df_all[f]) \n",
    "\n",
    "    ###\n",
    "    ###building_id, manager_id\n",
    "    ###\n",
    "    value_counts = df_all['building_id'].value_counts()\n",
    "    df_all = pd.merge(df_all, pd.DataFrame(value_counts), left_on='building_id', right_index=True).sort_index()\n",
    "    df_all = df_all.drop(['building_id_x'], axis=1)    \n",
    "    df_all.loc[df_all.building_id == 0, 'building_id_y'] = 0\n",
    "\n",
    "    value_counts = df_all['manager_id'].value_counts()\n",
    "    df_all = pd.merge(df_all, pd.DataFrame(value_counts), left_on='manager_id', right_index=True).sort_index()\n",
    "    df_all = df_all.drop(['manager_id_x'], axis=1)    \n",
    "    df_all.loc[df_all.manager_id == 0, 'manager_id_y'] = 0\n",
    "\n",
    "    print(df_all.shape)\n",
    "\n",
    "    #done\n",
    "    X_train = df_all[:df_train.shape[0]]\n",
    "    X_test = df_all[df_train.shape[0]:]\n",
    "\n",
    "    X_train = pd.concat((X_train, pd.DataFrame(tr_sparse.todense())), axis=1)\n",
    "    X_test = pd.concat((X_test, pd.DataFrame(te_sparse.todense())), axis=1)\n",
    "\n",
    "    #X_train = csr_matrix(np.hstack([X_train, tr_sparse.todense()]))\n",
    "    #X_test = csr_matrix(np.hstack([X_test, te_sparse.todense()]))\n",
    "    #X_train = csr_matrix(np.hstack([X_train, tr_sparse.todense(), tr_sparse2.todense()]))\n",
    "    #X_test = csr_matrix(np.hstack([X_test, te_sparse.todense(), te_sparse2.todense()]))\n",
    "\n",
    "    print('Train', X_train.shape)\n",
    "    print('Test', X_test.shape)\n",
    "\n",
    "    print('feature engineering done', datetime.datetime.now())\n",
    "    return X_train, X_test\n",
    "\n",
    "def feature_engineering_extra(df_train, df_test, y_train):\n",
    "    temp = pd.concat([df_train.manager_id, pd.get_dummies(y_train)], axis = 1).groupby('manager_id').mean()\n",
    "    temp.columns = ['high_frac', 'medium_frac', 'low_frac']\n",
    "    #this is equivalent of number of reviews\n",
    "    temp['manager_listings'] = df_train.groupby('manager_id').count().iloc[:,1]\n",
    "    #this is equivalent to star rating (0, 1 or 2 stars)\n",
    "    temp['manager_skill'] = temp['high_frac']*2 + temp['medium_frac']\n",
    "    #lower the rating for fewer listings\n",
    "    #temp['manager_skill'] = temp.manager_skill*expit((temp.manager_listings - 1)/4)\n",
    "    #temp['manager_skill'] = temp.manager_skill*expit(temp.manager_listings/4)\n",
    "\n",
    "    #use mean for managers with < 20 listings in train. TBD: explain why\n",
    "    unranked_managers_ixes = temp['manager_listings'] < 20\n",
    "    ranked_managers_ixes = ~unranked_managers_ixes\n",
    "    mean_values = temp.loc[ranked_managers_ixes, ['high_frac', 'medium_frac', 'low_frac', 'manager_skill']].mean()\n",
    "    temp.loc[unranked_managers_ixes, ['high_frac', 'medium_frac', 'low_frac', 'manager_skill']] = mean_values.values\n",
    "\n",
    "    temp = temp['manager_skill']\n",
    "    \n",
    "    #join\n",
    "    df_train = df_train.merge(temp.reset_index(), how='left', left_on='manager_id', right_on='manager_id')\n",
    "    #manager with no listing - give them default 0.5 rating\n",
    "    #df_all2 = df_all2.fillna(0.5)\n",
    "    #df_all2 = df_all2.fillna(0)\n",
    "    \n",
    "    #remove manager_id - score is worse\n",
    "    #df_train = df_train.drop(['manager_id'], axis=1)    \n",
    "       \n",
    "    #join\n",
    "    df_test = df_test.merge(temp.reset_index(), how='left', left_on='manager_id', right_on='manager_id')\n",
    "    #manager with no listing - give them default 0.5 rating\n",
    "    #df_all2 = df_all2.fillna(0.5)\n",
    "    #df_all2 = df_all2.fillna(0)\n",
    "    #use mean for managers with no listings. TBD: explain why\n",
    "    new_manager_ixes = df_test['manager_skill'].isnull()\n",
    "    df_test.loc[new_manager_ixes, 'manager_skill'] = mean_values['manager_skill']\n",
    "        \n",
    "    #remove manager_id - score is worse\n",
    "    #df_test = df_test.drop(['manager_id'], axis=1)    \n",
    "        \n",
    "    '''\n",
    "    temp = pd.concat([df_all[:df_train.shape[0]].building_id, pd.get_dummies(y_train)], axis = 1).groupby('building_id').mean()\n",
    "    temp.columns = ['high_frac', 'low_frac', 'medium_frac']\n",
    "    #this is equivalent of number of reviews\n",
    "    temp['building_listings'] = df_all[:df_train.shape[0]].groupby('building_id').count().iloc[:,1]\n",
    "    #this is equivalent to star rating (0, 1 or 2 stars)\n",
    "    temp['building_skill'] = temp['high_frac']*2 + temp['medium_frac']\n",
    "    #lower the rating for fewer listings\n",
    "    #temp['building_skill'] = temp.building_skill*expit((temp.building_listings - 1)/4)\n",
    "    #temp['building_skill'] = temp.building_skill*expit(temp.building_listings/4)\n",
    "        \n",
    "    #use mean for buildings with < 20 listings in train. TBD: explain why\n",
    "    unranked_managers_ixes = temp['building_listings'] < 20\n",
    "    ranked_managers_ixes = ~unranked_managers_ixes\n",
    "    mean_values = temp.loc[ranked_managers_ixes, ['high_frac','low_frac', 'medium_frac','building_skill']].mean()\n",
    "    temp.loc[unranked_managers_ixes,['high_frac','low_frac', 'medium_frac','building_skill']] = mean_values.values\n",
    "\n",
    "    temp = temp['building_skill']\n",
    "    #join\n",
    "    df_all2 = df_all.merge(temp.reset_index(), how='left', left_on='building_id', right_on='building_id')\n",
    "    #building with no listing - give them default 0.5 rating\n",
    "    #df_all2 = df_all2.fillna(0.5)\n",
    "    #df_all2 = df_all2.fillna(0)\n",
    "    #use mean for buidlings with no listings. TBD: explain why\n",
    "    df_all2 = df_all2.fillna(mean_values['building_skill'])\n",
    "    df_all['building_skill'] = df_all2['building_skill']\n",
    "\n",
    "    #remove building_id?\n",
    "    #df_all = df_all.drop(['building_id'], axis=1)    \n",
    "    '''\n",
    "\n",
    "    return df_train, df_test\n",
    "\n",
    "def my_cv(clf, X_train, y_train):\n",
    "    early_stopping_rounds = 100\n",
    "\n",
    "    xgb_options = clf.get_xgb_params()\n",
    "    xgb_options['num_class'] = 3\n",
    "    xgb_options.update({\"eval_metric\":'mlogloss'})\n",
    "    train_dmatrix = DMatrix(csr_matrix(X_train), label=y_train)\n",
    "\n",
    "    folds = StratifiedKFold(y_train, n_folds=5, shuffle=True)\n",
    "    cv_results = cv(xgb_options, train_dmatrix, clf.n_estimators, early_stopping_rounds=early_stopping_rounds, verbose_eval=False, show_stdv=False, folds=folds)\n",
    "\n",
    "    return cv_results.values[-1][0], cv_results.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleaning_text(text):\n",
    "    #sentence=sentence.lower()\n",
    "    text = text.replace('<p><a  website_redacted', '')\n",
    "    text = text.replace('!<br /><br />', '')\n",
    "    text = text.replace('kagglemanager renthop com', '')\n",
    "    text = re.sub('[^\\w\\s]',' ', text) #removes punctuations\n",
    "    text = re.sub('\\d+',' ', text) #removes digits\n",
    "    text =' '.join([w for w in text.split() if not w in ENGLISH_STOP_WORDS]) # removes english stopwords\n",
    "    #text=' '.join([w for w , pos in pos_tag(text.split()) if (pos == 'NN' or pos=='JJ' or pos=='JJR' or pos=='JJS' )])\n",
    "    #selecting only nouns and adjectives\n",
    "    text =' '.join([w for w in text.split() if not len(w)<=2 ]) #removes single lettered words and digits\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def cleaning_text2(text):\n",
    "    text = re.sub('[^\\w\\s]',' ', text) #removes punctuations\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def cleaning_list(list):\n",
    "    return [cleaning_text2(x) for x in list]\n",
    "    #return map(cleaning_text, list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ENGLISH_STOP_WORDS = frozenset([\n",
    "    \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\",\n",
    "    \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n",
    "    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\",\n",
    "    \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\",\n",
    "    \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "    \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\",\n",
    "    \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\",\n",
    "    \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\",\n",
    "    \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\",\n",
    "    \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\",\n",
    "    \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\",\n",
    "    \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\",\n",
    "    \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\",\n",
    "    \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
    "    \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
    "    \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\",\n",
    "    \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\",\n",
    "    \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\",\n",
    "    \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\",\n",
    "    \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\",\n",
    "    \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\",\n",
    "    \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n",
    "    \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\",\n",
    "    \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\",\n",
    "    \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\",\n",
    "    \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\",\n",
    "    \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\",\n",
    "    \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n",
    "    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\",\n",
    "    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n",
    "    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n",
    "    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n",
    "    \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "    \"yourselves\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature engineering 2017-03-10 10:26:49.735614\n",
      "zones (124011, 25)\n",
      "zones (124011, 165)\n",
      "(124011, 168)\n",
      "Train (49352, 1398)\n",
      "Test (124011, 1398)\n",
      "feature engineering done 2017-03-10 10:28:04.375846\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = feature_engineering(df_train, df_test, y_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test = feature_engineering_extra(X_train, X_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = csr_matrix(X_train.values)\n",
    "X_test = csr_matrix(X_test.values)\n",
    "\n",
    "learning_rate, max_depth, ss, cs, gamma, min_child_weight, reg_lambda, reg_alpha = 0.1, 6, 0.7, 0.7, 0, 1, 1, 0\n",
    "clf = XGBClassifier(max_depth=max_depth, learning_rate=learning_rate, n_estimators=344, objective='multi:softprob', subsample=ss, colsample_bytree=cs, gamma=gamma, min_child_weight=min_child_weight, reg_lambda=reg_lambda, reg_alpha=reg_alpha)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_predict = clf.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124011, 3)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74659, 14)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124011, 1399)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
