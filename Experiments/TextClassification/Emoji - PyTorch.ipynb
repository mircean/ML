{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "\n",
    "#glove_file = 'glove/glove.6B.50d.txt'\n",
    "#glove_dim = 50\n",
    "glove_file = 'glove/glove.840B.300d.txt'\n",
    "glove_dim = 300\n",
    "use_lemmatization = False\n",
    "\n",
    "cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    return unicodedata.normalize('NFD', text)\n",
    "\n",
    "def load_glove_vocab(wv_file, wv_dim):\n",
    "    vocab = set()\n",
    "    with open(wv_file, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            elems = line.split()\n",
    "            token = normalize_text(''.join(elems[0:-wv_dim]))\n",
    "            vocab.add(token)\n",
    "    return vocab\n",
    "\n",
    "def build_embedding(wv_file, wv_dim, target_vocab):\n",
    "    vocab_size = len(target_vocab)\n",
    "    emb = np.random.uniform(-1, 1, (vocab_size, wv_dim))\n",
    "    emb[0] = 0 # <PAD> should be all 0 (using broadcast)\n",
    "\n",
    "    w2id = {w: i for i, w in enumerate(target_vocab)}\n",
    "    with open(wv_file, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            elems = line.split()\n",
    "            token = normalize_text(''.join(elems[0:-wv_dim]))\n",
    "            if token == '<PAD>':\n",
    "                print(token)\n",
    "            if token in w2id:\n",
    "                emb[w2id[token]] = [float(v) for v in elems[-wv_dim:]]\n",
    "    return emb\n",
    "\n",
    "def token2id(docs, vocab, unk_id=None):\n",
    "    w2id = {w: i for i, w in enumerate(vocab)}\n",
    "    ids = [[w2id[w] if w in w2id else unk_id for w in doc] for doc in docs]\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove loaded.\n"
     ]
    }
   ],
   "source": [
    "glove_vocab = load_glove_vocab(glove_file, glove_dim) \n",
    "print('glove loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2195836"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'sign in' in glove_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finely-ground\n",
      "HIGH-QUALITY\n",
      "64-41\n",
      "44-years-old\n",
      "06-01-2009\n",
      "Eight-channel\n",
      "B-Flex\n",
      "non-judgement\n",
      "super-secretive\n",
      "Euro-zone\n",
      "-5.45\n",
      "Procure-to-Pay\n",
      "Verse-by-Verse\n",
      "Sixty-Fifth\n",
      "Double-Coated\n",
      "02-08-2005\n",
      "01-Mar-2011\n",
      "14-01-2010\n",
      "1-5x\n",
      "WF-2540\n",
      "dat-sick\n",
      "now-destroyed\n",
      "24-Apr-2005\n",
      "THIRTY-FIVE\n",
      "inter-element\n",
      "q-o-q\n",
      "2006-01-23\n",
      "62-35\n",
      "suitcase-style\n",
      "non-renewables\n",
      "360-Degree\n",
      "co-organizer\n",
      "-3177\n",
      "30-Jul-2006\n",
      "-4206\n",
      "5-Yr\n",
      "Tri-power\n",
      "1878-10-25\n",
      "1798-1800\n",
      "e-code\n",
      "101-106\n",
      "candy-coat\n",
      "Medi-Pak\n",
      "0.93-1\n",
      "well-rounded\n",
      "RG-174\n",
      "out-of-memory\n",
      "Easy-to-learn\n",
      "11-17-08\n",
      "20th-Feb-2013\n",
      "psycho-therapeutic\n",
      "000-M16\n",
      "Heart-Attack\n",
      "time-pass\n",
      "single-hit\n",
      "Aug-10-2012\n",
      "n-gon\n",
      "anti-surge\n",
      "Greensboro-High\n",
      "Find-Locksmith\n",
      "British-themed\n",
      "2009-09-20\n",
      "54-year-olds\n",
      "408-410\n",
      "Light-heavyweight\n",
      "-4151\n",
      "2-for-17\n",
      "free-thought\n",
      "Self-Immolation\n",
      "one-subject\n",
      "celui-ci\n",
      "smart-alecky\n",
      "TL-01\n",
      "foster-care\n",
      "X-Alp\n",
      "League-Wide\n",
      "above-average\n",
      "4-core\n",
      "backed-up\n",
      "ragged-looking\n",
      "Short-id\n",
      "dma-mapping\n",
      "bottle05-20-2013\n",
      "US-6\n",
      "173-186\n",
      "www.china-led-fluorescent-tube.com/PRODUCT/LANG-af/led\n",
      "Aug-10-09\n",
      "early-70\n",
      "on-the-cheap\n",
      "one-question\n",
      "anti-youth\n",
      "31-May\n",
      "model-specific\n",
      "9-Channel\n",
      "stick-shift\n",
      "environ-mental\n",
      "dog-lovers\n",
      "S/T-SERIES\n",
      "non-spermicidal\n",
      "MIL-PRF-19500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for x in glove_vocab:\n",
    "    if '-' in x:\n",
    "        print(x)\n",
    "        count += 1\n",
    "        if count == 100: break\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am proud of your achievements']"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_train = []\n",
    "#docs_raw.append('Based on fully-aware attention, we propose an end-to-end architecture qaz123')\n",
    "#docs_raw.append('Teaching machines to read, process and comprehend text and then answer questions is one of key problems in artificial intelligence')\n",
    "docs_train.append('I am proud of your achievements')\n",
    "docs_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>never talk to me again</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am proud of your achievements</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It is the worst day in my life</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Miss you so much</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>food is life</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I love you mum</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Stop saying bullshit</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>congratulations on your acceptance</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The assignment is too long</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I want to go play</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>she did not answer my text</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Your stupidity has no limit</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>how many points did he score</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>my algorithm performs poorly</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I got approved</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Stop shouting at me</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sounds like a fun plan ha ha</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>no one likes him</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>the game just finished</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>I will celebrate soon</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>So sad you are not coming</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>She is my dearest love</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Good job</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>It was funny lol</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>candy is life</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>The chicago cubs won again</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>I am hungry</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>I am so excited to see you after so long</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>you did well on you exam</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>lets brunch some day</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Good joke</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>This specialization is great</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>you could not solve it</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>I am so happy for you</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Congrats on the new job</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>I am proud of you forever</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>I want to eat</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>That catcher sucks</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>The first base man got the ball</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>this is bad</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>you did not do your homework</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>I will have a cheese cake</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>do you have a ball</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>the lectures are great though</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Are you down for baseball this afternoon</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>what are the rules of the game</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>I am always working</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>where is the stadium</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>She is the cutest person I have ever seen</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>vegetables are healthy</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>he is handsome</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>too bad that you were not here</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>you are a loser</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>I love indian food</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>Who is down for a restaurant</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>he had to make a home run</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>I am ordering food</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>What is wrong with you</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>I love you</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>great job</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             0  1   2     3\n",
       "0                       never talk to me again  3 NaN   NaN\n",
       "1              I am proud of your achievements  2 NaN   NaN\n",
       "2               It is the worst day in my life  3 NaN   NaN\n",
       "3                             Miss you so much  0 NaN   [0]\n",
       "4                                 food is life  4 NaN   NaN\n",
       "5                               I love you mum  0 NaN   NaN\n",
       "6                         Stop saying bullshit  3 NaN   NaN\n",
       "7           congratulations on your acceptance  2 NaN   NaN\n",
       "8                  The assignment is too long   3 NaN   NaN\n",
       "9                            I want to go play  1 NaN   [3]\n",
       "10                 she did not answer my text   3 NaN   NaN\n",
       "11                 Your stupidity has no limit  3 NaN   NaN\n",
       "12                how many points did he score  1 NaN   NaN\n",
       "13                my algorithm performs poorly  3 NaN   NaN\n",
       "14                              I got approved  2 NaN   NaN\n",
       "15                         Stop shouting at me  3 NaN   NaN\n",
       "16                Sounds like a fun plan ha ha  2 NaN   NaN\n",
       "17                            no one likes him  3 NaN   NaN\n",
       "18                      the game just finished  1 NaN   [2]\n",
       "19                       I will celebrate soon  2 NaN   NaN\n",
       "20                   So sad you are not coming  3 NaN   NaN\n",
       "21                      She is my dearest love  0 NaN   [1]\n",
       "22                                    Good job  2 NaN   [4]\n",
       "23                            It was funny lol  2 NaN   NaN\n",
       "24                              candy is life   2 NaN   NaN\n",
       "25                  The chicago cubs won again  1 NaN   NaN\n",
       "26                                 I am hungry  4 NaN   NaN\n",
       "27    I am so excited to see you after so long  2 NaN   NaN\n",
       "28                    you did well on you exam  2 NaN   NaN\n",
       "29                        lets brunch some day  4 NaN   NaN\n",
       "..                                         ... ..  ..   ...\n",
       "102                                  Good joke  2 NaN   NaN\n",
       "103               This specialization is great  2 NaN   NaN\n",
       "104                     you could not solve it  3 NaN   NaN\n",
       "105                      I am so happy for you  2 NaN   NaN\n",
       "106                    Congrats on the new job  2 NaN   NaN\n",
       "107                  I am proud of you forever  2 NaN   NaN\n",
       "108                              I want to eat  4 NaN   NaN\n",
       "109                        That catcher sucks   1 NaN   NaN\n",
       "110            The first base man got the ball  1 NaN   NaN\n",
       "111                                this is bad  3 NaN   NaN\n",
       "112               you did not do your homework  3 NaN   NaN\n",
       "113                  I will have a cheese cake  4 NaN   NaN\n",
       "114                         do you have a ball  1 NaN   NaN\n",
       "115             the lectures are great though   2 NaN   NaN\n",
       "116   Are you down for baseball this afternoon  1 NaN   NaN\n",
       "117             what are the rules of the game  1 NaN   NaN\n",
       "118                        I am always working  3 NaN   NaN\n",
       "119                       where is the stadium  1 NaN   NaN\n",
       "120  She is the cutest person I have ever seen  0 NaN   [4]\n",
       "121                     vegetables are healthy  4 NaN   NaN\n",
       "122                             he is handsome  0 NaN   NaN\n",
       "123             too bad that you were not here  3 NaN   NaN\n",
       "124                            you are a loser  3 NaN   NaN\n",
       "125                         I love indian food  4 NaN   NaN\n",
       "126               Who is down for a restaurant  4 NaN   NaN\n",
       "127                  he had to make a home run  1 NaN   NaN\n",
       "128                         I am ordering food  4 NaN   NaN\n",
       "129                     What is wrong with you  3 NaN   NaN\n",
       "130                                 I love you  0 NaN   NaN\n",
       "131                                  great job  2 NaN   NaN\n",
       "\n",
       "[132 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('emoji/train_emoji.csv', header=None)\n",
    "docs_train = df_train[0]\n",
    "Y_train = Variable(torch.LongTensor(df_train[1].values), requires_grad=False)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I want to eat\\t</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>he did not answer\\t</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>he got a very nice raise\\t</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>she got me a nice present\\t</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ha ha ha it was so funny\\t</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>he is a good friend\\t</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I am upset\\t</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>We had such a lovely dinner tonight\\t</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>where is the food\\t</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Stop making this joke ha ha ha\\t</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>where is the ball\\t</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>work is hard\\t</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>This girl is messing with me\\t</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>are you serious</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Let us go play baseball\\t</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>This stupid grader is not working \\t</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>work is horrible\\t</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Congratulation for having a baby\\t</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>stop pissing me off</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>any suggestions for dinner\\t</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>I love taking breaks\\t</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>you brighten my day\\t</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>I boiled rice\\t</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>she is a bully\\t</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Why are you feeling bad\\t</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>I am upset\\t</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>give me the ball</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>My grandmother is the love of my life\\t</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>enjoy your game</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>valentine day is near\\t</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>I miss you so much\\t</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>throw the ball\\t</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>My life is so boring\\t</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>she said yes\\t</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>will you be my valentine\\t</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>he can pitch really well\\t</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>dance with me\\t</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>I am hungry</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>See you at the restaurant\\t</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>I like to laugh\\t</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>I will  run</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>I like your jacket \\t</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>i miss her\\t</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>what is your favorite baseball game\\t</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Good job\\t</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>I love you to the stars and back\\t</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>What you did was awesome\\t</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>ha ha ha lol\\t</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>I do not want to joke\\t</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>go away\\t</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>yesterday we lost again\\t</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>family is all I have\\t</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>you are failing this exercise\\t</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Good joke\\t</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>You deserve this nice prize\\t</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>I did not have breakfast</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          0  1\n",
       "0                           I want to eat\\t  4\n",
       "1                       he did not answer\\t  3\n",
       "2                he got a very nice raise\\t  2\n",
       "3               she got me a nice present\\t  2\n",
       "4                ha ha ha it was so funny\\t  2\n",
       "5                     he is a good friend\\t  2\n",
       "6                              I am upset\\t  3\n",
       "7     We had such a lovely dinner tonight\\t  2\n",
       "8                       where is the food\\t  4\n",
       "9          Stop making this joke ha ha ha\\t  2\n",
       "10                      where is the ball\\t  1\n",
       "11                           work is hard\\t  3\n",
       "12           This girl is messing with me\\t  3\n",
       "13                          are you serious  3\n",
       "14                Let us go play baseball\\t  1\n",
       "15     This stupid grader is not working \\t  3\n",
       "16                       work is horrible\\t  3\n",
       "17       Congratulation for having a baby\\t  2\n",
       "18                      stop pissing me off  3\n",
       "19             any suggestions for dinner\\t  4\n",
       "20                   I love taking breaks\\t  0\n",
       "21                    you brighten my day\\t  2\n",
       "22                          I boiled rice\\t  4\n",
       "23                         she is a bully\\t  3\n",
       "24                Why are you feeling bad\\t  3\n",
       "25                             I am upset\\t  3\n",
       "26                         give me the ball  1\n",
       "27  My grandmother is the love of my life\\t  0\n",
       "28                          enjoy your game  1\n",
       "29                  valentine day is near\\t  2\n",
       "30                     I miss you so much\\t  0\n",
       "31                         throw the ball\\t  1\n",
       "32                   My life is so boring\\t  3\n",
       "33                           she said yes\\t  2\n",
       "34               will you be my valentine\\t  2\n",
       "35               he can pitch really well\\t  1\n",
       "36                          dance with me\\t  2\n",
       "37                              I am hungry  4\n",
       "38              See you at the restaurant\\t  4\n",
       "39                        I like to laugh\\t  2\n",
       "40                              I will  run  1\n",
       "41                    I like your jacket \\t  0\n",
       "42                             i miss her\\t  0\n",
       "43    what is your favorite baseball game\\t  1\n",
       "44                               Good job\\t  2\n",
       "45       I love you to the stars and back\\t  0\n",
       "46               What you did was awesome\\t  2\n",
       "47                           ha ha ha lol\\t  2\n",
       "48                  I do not want to joke\\t  3\n",
       "49                                go away\\t  3\n",
       "50                yesterday we lost again\\t  3\n",
       "51                   family is all I have\\t  0\n",
       "52          you are failing this exercise\\t  3\n",
       "53                              Good joke\\t  2\n",
       "54            You deserve this nice prize\\t  2\n",
       "55                I did not have breakfast   4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('emoji/tesss.csv', header=None)\n",
    "docs_test = df_test[0]\n",
    "Y_test = Variable(torch.LongTensor(df_test[1].values), requires_grad=False)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "never never ADV  O\n",
      "talk talk VERB  O\n",
      "to to ADP  O\n",
      "me me PRON  O\n",
      "again again ADV  O\n"
     ]
    }
   ],
   "source": [
    "nlp_doc = nlp(docs_train[0])\n",
    "for token in nlp_doc:\n",
    "    if not token.is_punct and not token.is_space:\n",
    "        print(token.text, token.lemma_ if token.lemma_ != '-PRON-' else token.lower_ , token.pos_, token.ent_type_, token.ent_iob_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['never', 'talk', 'to', 'me', 'again'],\n",
       " ['i', 'am', 'proud', 'of', 'your', 'achievements'],\n",
       " ['it', 'is', 'the', 'worst', 'day', 'in', 'my', 'life'],\n",
       " ['miss', 'you', 'so', 'much'],\n",
       " ['food', 'is', 'life'],\n",
       " ['i', 'love', 'you', 'mum'],\n",
       " ['stop', 'saying', 'bullshit'],\n",
       " ['congratulations', 'on', 'your', 'acceptance'],\n",
       " ['the', 'assignment', 'is', 'too', 'long'],\n",
       " ['i', 'want', 'to', 'go', 'play'],\n",
       " ['she', 'did', 'not', 'answer', 'my', 'text'],\n",
       " ['your', 'stupidity', 'has', 'no', 'limit'],\n",
       " ['how', 'many', 'points', 'did', 'he', 'score'],\n",
       " ['my', 'algorithm', 'performs', 'poorly'],\n",
       " ['i', 'got', 'approved'],\n",
       " ['stop', 'shouting', 'at', 'me'],\n",
       " ['sounds', 'like', 'a', 'fun', 'plan', 'ha', 'ha'],\n",
       " ['no', 'one', 'likes', 'him'],\n",
       " ['the', 'game', 'just', 'finished'],\n",
       " ['i', 'will', 'celebrate', 'soon'],\n",
       " ['so', 'sad', 'you', 'are', 'not', 'coming'],\n",
       " ['she', 'is', 'my', 'dearest', 'love'],\n",
       " ['good', 'job'],\n",
       " ['it', 'was', 'funny', 'lol'],\n",
       " ['candy', 'is', 'life'],\n",
       " ['the', 'chicago', 'cubs', 'won', 'again'],\n",
       " ['i', 'am', 'hungry'],\n",
       " ['i', 'am', 'so', 'excited', 'to', 'see', 'you', 'after', 'so', 'long'],\n",
       " ['you', 'did', 'well', 'on', 'you', 'exam'],\n",
       " ['lets', 'brunch', 'some', 'day'],\n",
       " ['he', 'is', 'so', 'cute'],\n",
       " ['how', 'dare', 'you', 'ask', 'that'],\n",
       " ['do', 'you', 'want', 'to', 'join', 'me', 'for', 'dinner'],\n",
       " ['i', 'said', 'yes'],\n",
       " ['she', 'is', 'attractive'],\n",
       " ['you', 'suck'],\n",
       " ['she', 'smiles', 'a', 'lot'],\n",
       " ['he', 'is', 'laughing'],\n",
       " ['she', 'takes', 'forever', 'to', 'get', 'ready'],\n",
       " ['french', 'macaroon', 'is', 'so', 'tasty'],\n",
       " ['we', 'made', 'it'],\n",
       " ['i', 'am', 'excited'],\n",
       " ['i', 'adore', 'my', 'dogs'],\n",
       " ['congratulations'],\n",
       " ['this', 'girl', 'was', 'mean'],\n",
       " ['you', 'two', 'are', 'cute'],\n",
       " ['my', 'code', 'is', 'working', 'but', 'the', 'grader', 'gave', 'me', 'zero'],\n",
       " ['this', 'joke', 'is', 'killing', 'me', 'haha'],\n",
       " ['do', 'you', 'like', 'pizza'],\n",
       " ['you', 'got', 'a', 'down', 'grade'],\n",
       " ['i', 'missed', 'you'],\n",
       " ['i', 'think', 'i', 'will', 'end', 'up', 'alone'],\n",
       " ['i', 'got', 'humiliated', 'by', 'my', 'sister'],\n",
       " ['you', 'are', 'awful'],\n",
       " ['i', 'cooked', 'meat'],\n",
       " ['this', 'is', 'so', 'funny'],\n",
       " ['lets', 'exercise'],\n",
       " ['he', 'is', 'the', 'best', 'player'],\n",
       " ['i', 'am', 'going', 'to', 'the', 'stadium'],\n",
       " ['you', 'are', 'incredibly', 'intelligent', 'and', 'talented'],\n",
       " ['stop', 'shouting', 'at', 'me'],\n",
       " ['who', 'is', 'your', 'favorite', 'player'],\n",
       " ['i', 'like', 'you', 'a', 'lot'],\n",
       " ['i', 'miss', 'him'],\n",
       " ['my', 'dog', 'just', 'had', 'a', 'few', 'puppies'],\n",
       " ['i', 'hate', 'him'],\n",
       " ['i', 'want', 'chinese', 'food'],\n",
       " ['cookies', 'are', 'good'],\n",
       " ['her', 'smile', 'is', 'so', 'charming'],\n",
       " ['bravo',\n",
       "  'for',\n",
       "  'the',\n",
       "  'announcement',\n",
       "  'it',\n",
       "  'got',\n",
       "  'a',\n",
       "  'lot',\n",
       "  'of',\n",
       "  'traction'],\n",
       " ['she', 'plays', 'baseball'],\n",
       " ['he', 'did', 'an', 'amazing', 'job'],\n",
       " ['the', 'baby', 'is', 'adorable'],\n",
       " ['i', 'was', 'waiting', 'for', 'her', 'for', 'two', 'hours'],\n",
       " ['funny'],\n",
       " ['i', 'like', 'it', 'when', 'people', 'smile'],\n",
       " ['i', 'love', 'dogs'],\n",
       " ['they', 'are', 'so', 'kind', 'and', 'friendly'],\n",
       " ['so', 'bad', 'that', 'you', 'can', 'not', 'come', 'with', 'us'],\n",
       " ['he', 'likes', 'baseball'],\n",
       " ['i',\n",
       "  'am',\n",
       "  'so',\n",
       "  'impressed',\n",
       "  'by',\n",
       "  'your',\n",
       "  'dedication',\n",
       "  'to',\n",
       "  'this',\n",
       "  'project'],\n",
       " ['i', 'am', 'at', 'the', 'baseball', 'game'],\n",
       " ['bravo'],\n",
       " ['what', 'a', 'fun', 'moment'],\n",
       " ['i', 'want', 'to', 'have', 'sushi', 'for', 'dinner'],\n",
       " ['i', 'am', 'very', 'disappointed'],\n",
       " ['he', 'can', 'not', 'do', 'anything'],\n",
       " ['lol'],\n",
       " ['lets', 'have', 'food', 'together'],\n",
       " ['she', 'is', 'so', 'cute'],\n",
       " ['miss', 'you', 'my', 'dear'],\n",
       " ['i', 'am', 'looking', 'for', 'a', 'date'],\n",
       " ['i', 'am', 'frustrated'],\n",
       " ['i', 'lost', 'my', 'wallet'],\n",
       " ['you', 'failed', 'the', 'midterm'],\n",
       " ['ha', 'ha', 'ha', 'it', 'was', 'so', 'funny'],\n",
       " ['do', 'you', 'want', 'to', 'give', 'me', 'a', 'hug'],\n",
       " ['who', 'is', 'playing', 'in', 'the', 'final'],\n",
       " ['she', 'is', 'happy'],\n",
       " ['you', 'are', 'not', 'qualified', 'for', 'this', 'position'],\n",
       " ['i', 'love', 'my', 'dad'],\n",
       " ['this', 'guy', 'was', 'such', 'a', 'joke'],\n",
       " ['good', 'joke'],\n",
       " ['this', 'specialization', 'is', 'great'],\n",
       " ['you', 'could', 'not', 'solve', 'it'],\n",
       " ['i', 'am', 'so', 'happy', 'for', 'you'],\n",
       " ['congrats', 'on', 'the', 'new', 'job'],\n",
       " ['i', 'am', 'proud', 'of', 'you', 'forever'],\n",
       " ['i', 'want', 'to', 'eat'],\n",
       " ['that', 'catcher', 'sucks'],\n",
       " ['the', 'first', 'base', 'man', 'got', 'the', 'ball'],\n",
       " ['this', 'is', 'bad'],\n",
       " ['you', 'did', 'not', 'do', 'your', 'homework'],\n",
       " ['i', 'will', 'have', 'a', 'cheese', 'cake'],\n",
       " ['do', 'you', 'have', 'a', 'ball'],\n",
       " ['the', 'lectures', 'are', 'great', 'though'],\n",
       " ['are', 'you', 'down', 'for', 'baseball', 'this', 'afternoon'],\n",
       " ['what', 'are', 'the', 'rules', 'of', 'the', 'game'],\n",
       " ['i', 'am', 'always', 'working'],\n",
       " ['where', 'is', 'the', 'stadium'],\n",
       " ['she', 'is', 'the', 'cutest', 'person', 'i', 'have', 'ever', 'seen'],\n",
       " ['vegetables', 'are', 'healthy'],\n",
       " ['he', 'is', 'handsome'],\n",
       " ['too', 'bad', 'that', 'you', 'were', 'not', 'here'],\n",
       " ['you', 'are', 'a', 'loser'],\n",
       " ['i', 'love', 'indian', 'food'],\n",
       " ['who', 'is', 'down', 'for', 'a', 'restaurant'],\n",
       " ['he', 'had', 'to', 'make', 'a', 'home', 'run'],\n",
       " ['i', 'am', 'ordering', 'food'],\n",
       " ['what', 'is', 'wrong', 'with', 'you'],\n",
       " ['i', 'love', 'you'],\n",
       " ['great', 'job']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_train_tokens = []\n",
    "for doc in docs_train:\n",
    "    nlp_doc = nlp(doc)\n",
    "    #keep stop words \n",
    "    #spacy doesn't have stemming, only lemmatization\n",
    "    if use_lemmatization:\n",
    "        tokens = [token.lemma_ if token.lemma_ != '-PRON-' else token.lower_ for token in nlp_doc if not token.is_punct and not token.is_space]\n",
    "    else:\n",
    "        tokens = [token.lower_ for token in nlp_doc if not token.is_punct and not token.is_space]\n",
    "    docs_train_tokens.append(tokens)\n",
    "\n",
    "docs_train_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'want', 'to', 'eat'],\n",
       " ['he', 'did', 'not', 'answer'],\n",
       " ['he', 'got', 'a', 'very', 'nice', 'raise'],\n",
       " ['she', 'got', 'me', 'a', 'nice', 'present'],\n",
       " ['ha', 'ha', 'ha', 'it', 'was', 'so', 'funny'],\n",
       " ['he', 'is', 'a', 'good', 'friend'],\n",
       " ['i', 'am', 'upset'],\n",
       " ['we', 'had', 'such', 'a', 'lovely', 'dinner', 'tonight'],\n",
       " ['where', 'is', 'the', 'food'],\n",
       " ['stop', 'making', 'this', 'joke', 'ha', 'ha', 'ha'],\n",
       " ['where', 'is', 'the', 'ball'],\n",
       " ['work', 'is', 'hard'],\n",
       " ['this', 'girl', 'is', 'messing', 'with', 'me'],\n",
       " ['are', 'you', 'serious'],\n",
       " ['let', 'us', 'go', 'play', 'baseball'],\n",
       " ['this', 'stupid', 'grader', 'is', 'not', 'working'],\n",
       " ['work', 'is', 'horrible'],\n",
       " ['congratulation', 'for', 'having', 'a', 'baby'],\n",
       " ['stop', 'pissing', 'me', 'off'],\n",
       " ['any', 'suggestions', 'for', 'dinner'],\n",
       " ['i', 'love', 'taking', 'breaks'],\n",
       " ['you', 'brighten', 'my', 'day'],\n",
       " ['i', 'boiled', 'rice'],\n",
       " ['she', 'is', 'a', 'bully'],\n",
       " ['why', 'are', 'you', 'feeling', 'bad'],\n",
       " ['i', 'am', 'upset'],\n",
       " ['give', 'me', 'the', 'ball'],\n",
       " ['my', 'grandmother', 'is', 'the', 'love', 'of', 'my', 'life'],\n",
       " ['enjoy', 'your', 'game'],\n",
       " ['valentine', 'day', 'is', 'near'],\n",
       " ['i', 'miss', 'you', 'so', 'much'],\n",
       " ['throw', 'the', 'ball'],\n",
       " ['my', 'life', 'is', 'so', 'boring'],\n",
       " ['she', 'said', 'yes'],\n",
       " ['will', 'you', 'be', 'my', 'valentine'],\n",
       " ['he', 'can', 'pitch', 'really', 'well'],\n",
       " ['dance', 'with', 'me'],\n",
       " ['i', 'am', 'hungry'],\n",
       " ['see', 'you', 'at', 'the', 'restaurant'],\n",
       " ['i', 'like', 'to', 'laugh'],\n",
       " ['i', 'will', 'run'],\n",
       " ['i', 'like', 'your', 'jacket'],\n",
       " ['i', 'miss', 'her'],\n",
       " ['what', 'is', 'your', 'favorite', 'baseball', 'game'],\n",
       " ['good', 'job'],\n",
       " ['i', 'love', 'you', 'to', 'the', 'stars', 'and', 'back'],\n",
       " ['what', 'you', 'did', 'was', 'awesome'],\n",
       " ['ha', 'ha', 'ha', 'lol'],\n",
       " ['i', 'do', 'not', 'want', 'to', 'joke'],\n",
       " ['go', 'away'],\n",
       " ['yesterday', 'we', 'lost', 'again'],\n",
       " ['family', 'is', 'all', 'i', 'have'],\n",
       " ['you', 'are', 'failing', 'this', 'exercise'],\n",
       " ['good', 'joke'],\n",
       " ['you', 'deserve', 'this', 'nice', 'prize'],\n",
       " ['i', 'did', 'not', 'have', 'breakfast']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_test_tokens = []\n",
    "for doc in docs_test:\n",
    "    nlp_doc = nlp(doc)\n",
    "    if use_lemmatization:\n",
    "        tokens = [token.lemma_ if token.lemma_ != '-PRON-' else token.lower_ for token in nlp_doc if not token.is_punct and not token.is_space]\n",
    "    else:\n",
    "        tokens = [token.lower_ for token in nlp_doc if not token.is_punct and not token.is_space]\n",
    "    docs_test_tokens.append(tokens)\n",
    "\n",
    "docs_test_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['never', 'talk', 'to', 'me', 'again'],\n",
       " ['i', 'am', 'proud', 'of', 'your', 'achievements'],\n",
       " ['it', 'is', 'the', 'worst', 'day', 'in', 'my', 'life'],\n",
       " ['miss', 'you', 'so', 'much'],\n",
       " ['food', 'is', 'life'],\n",
       " ['i', 'love', 'you', 'mum'],\n",
       " ['stop', 'saying', 'bullshit'],\n",
       " ['congratulations', 'on', 'your', 'acceptance'],\n",
       " ['the', 'assignment', 'is', 'too', 'long'],\n",
       " ['i', 'want', 'to', 'go', 'play'],\n",
       " ['she', 'did', 'not', 'answer', 'my', 'text'],\n",
       " ['your', 'stupidity', 'has', 'no', 'limit'],\n",
       " ['how', 'many', 'points', 'did', 'he', 'score'],\n",
       " ['my', 'algorithm', 'performs', 'poorly'],\n",
       " ['i', 'got', 'approved'],\n",
       " ['stop', 'shouting', 'at', 'me'],\n",
       " ['sounds', 'like', 'a', 'fun', 'plan', 'ha', 'ha'],\n",
       " ['no', 'one', 'likes', 'him'],\n",
       " ['the', 'game', 'just', 'finished'],\n",
       " ['i', 'will', 'celebrate', 'soon'],\n",
       " ['so', 'sad', 'you', 'are', 'not', 'coming'],\n",
       " ['she', 'is', 'my', 'dearest', 'love'],\n",
       " ['good', 'job'],\n",
       " ['it', 'was', 'funny', 'lol'],\n",
       " ['candy', 'is', 'life'],\n",
       " ['the', 'chicago', 'cubs', 'won', 'again'],\n",
       " ['i', 'am', 'hungry'],\n",
       " ['i', 'am', 'so', 'excited', 'to', 'see', 'you', 'after', 'so', 'long'],\n",
       " ['you', 'did', 'well', 'on', 'you', 'exam'],\n",
       " ['lets', 'brunch', 'some', 'day'],\n",
       " ['he', 'is', 'so', 'cute'],\n",
       " ['how', 'dare', 'you', 'ask', 'that'],\n",
       " ['do', 'you', 'want', 'to', 'join', 'me', 'for', 'dinner'],\n",
       " ['i', 'said', 'yes'],\n",
       " ['she', 'is', 'attractive'],\n",
       " ['you', 'suck'],\n",
       " ['she', 'smiles', 'a', 'lot'],\n",
       " ['he', 'is', 'laughing'],\n",
       " ['she', 'takes', 'forever', 'to', 'get', 'ready'],\n",
       " ['french', 'macaroon', 'is', 'so', 'tasty'],\n",
       " ['we', 'made', 'it'],\n",
       " ['i', 'am', 'excited'],\n",
       " ['i', 'adore', 'my', 'dogs'],\n",
       " ['congratulations'],\n",
       " ['this', 'girl', 'was', 'mean'],\n",
       " ['you', 'two', 'are', 'cute'],\n",
       " ['my', 'code', 'is', 'working', 'but', 'the', 'grader', 'gave', 'me', 'zero'],\n",
       " ['this', 'joke', 'is', 'killing', 'me', 'haha'],\n",
       " ['do', 'you', 'like', 'pizza'],\n",
       " ['you', 'got', 'a', 'down', 'grade'],\n",
       " ['i', 'missed', 'you'],\n",
       " ['i', 'think', 'i', 'will', 'end', 'up', 'alone'],\n",
       " ['i', 'got', 'humiliated', 'by', 'my', 'sister'],\n",
       " ['you', 'are', 'awful'],\n",
       " ['i', 'cooked', 'meat'],\n",
       " ['this', 'is', 'so', 'funny'],\n",
       " ['lets', 'exercise'],\n",
       " ['he', 'is', 'the', 'best', 'player'],\n",
       " ['i', 'am', 'going', 'to', 'the', 'stadium'],\n",
       " ['you', 'are', 'incredibly', 'intelligent', 'and', 'talented'],\n",
       " ['stop', 'shouting', 'at', 'me'],\n",
       " ['who', 'is', 'your', 'favorite', 'player'],\n",
       " ['i', 'like', 'you', 'a', 'lot'],\n",
       " ['i', 'miss', 'him'],\n",
       " ['my', 'dog', 'just', 'had', 'a', 'few', 'puppies'],\n",
       " ['i', 'hate', 'him'],\n",
       " ['i', 'want', 'chinese', 'food'],\n",
       " ['cookies', 'are', 'good'],\n",
       " ['her', 'smile', 'is', 'so', 'charming'],\n",
       " ['bravo',\n",
       "  'for',\n",
       "  'the',\n",
       "  'announcement',\n",
       "  'it',\n",
       "  'got',\n",
       "  'a',\n",
       "  'lot',\n",
       "  'of',\n",
       "  'traction'],\n",
       " ['she', 'plays', 'baseball'],\n",
       " ['he', 'did', 'an', 'amazing', 'job'],\n",
       " ['the', 'baby', 'is', 'adorable'],\n",
       " ['i', 'was', 'waiting', 'for', 'her', 'for', 'two', 'hours'],\n",
       " ['funny'],\n",
       " ['i', 'like', 'it', 'when', 'people', 'smile'],\n",
       " ['i', 'love', 'dogs'],\n",
       " ['they', 'are', 'so', 'kind', 'and', 'friendly'],\n",
       " ['so', 'bad', 'that', 'you', 'can', 'not', 'come', 'with', 'us'],\n",
       " ['he', 'likes', 'baseball'],\n",
       " ['i',\n",
       "  'am',\n",
       "  'so',\n",
       "  'impressed',\n",
       "  'by',\n",
       "  'your',\n",
       "  'dedication',\n",
       "  'to',\n",
       "  'this',\n",
       "  'project'],\n",
       " ['i', 'am', 'at', 'the', 'baseball', 'game'],\n",
       " ['bravo'],\n",
       " ['what', 'a', 'fun', 'moment'],\n",
       " ['i', 'want', 'to', 'have', 'sushi', 'for', 'dinner'],\n",
       " ['i', 'am', 'very', 'disappointed'],\n",
       " ['he', 'can', 'not', 'do', 'anything'],\n",
       " ['lol'],\n",
       " ['lets', 'have', 'food', 'together'],\n",
       " ['she', 'is', 'so', 'cute'],\n",
       " ['miss', 'you', 'my', 'dear'],\n",
       " ['i', 'am', 'looking', 'for', 'a', 'date'],\n",
       " ['i', 'am', 'frustrated'],\n",
       " ['i', 'lost', 'my', 'wallet'],\n",
       " ['you', 'failed', 'the', 'midterm'],\n",
       " ['ha', 'ha', 'ha', 'it', 'was', 'so', 'funny'],\n",
       " ['do', 'you', 'want', 'to', 'give', 'me', 'a', 'hug'],\n",
       " ['who', 'is', 'playing', 'in', 'the', 'final'],\n",
       " ['she', 'is', 'happy'],\n",
       " ['you', 'are', 'not', 'qualified', 'for', 'this', 'position'],\n",
       " ['i', 'love', 'my', 'dad'],\n",
       " ['this', 'guy', 'was', 'such', 'a', 'joke'],\n",
       " ['good', 'joke'],\n",
       " ['this', 'specialization', 'is', 'great'],\n",
       " ['you', 'could', 'not', 'solve', 'it'],\n",
       " ['i', 'am', 'so', 'happy', 'for', 'you'],\n",
       " ['congrats', 'on', 'the', 'new', 'job'],\n",
       " ['i', 'am', 'proud', 'of', 'you', 'forever'],\n",
       " ['i', 'want', 'to', 'eat'],\n",
       " ['that', 'catcher', 'sucks'],\n",
       " ['the', 'first', 'base', 'man', 'got', 'the', 'ball'],\n",
       " ['this', 'is', 'bad'],\n",
       " ['you', 'did', 'not', 'do', 'your', 'homework'],\n",
       " ['i', 'will', 'have', 'a', 'cheese', 'cake'],\n",
       " ['do', 'you', 'have', 'a', 'ball'],\n",
       " ['the', 'lectures', 'are', 'great', 'though'],\n",
       " ['are', 'you', 'down', 'for', 'baseball', 'this', 'afternoon'],\n",
       " ['what', 'are', 'the', 'rules', 'of', 'the', 'game'],\n",
       " ['i', 'am', 'always', 'working'],\n",
       " ['where', 'is', 'the', 'stadium'],\n",
       " ['she', 'is', 'the', 'cutest', 'person', 'i', 'have', 'ever', 'seen'],\n",
       " ['vegetables', 'are', 'healthy'],\n",
       " ['he', 'is', 'handsome'],\n",
       " ['too', 'bad', 'that', 'you', 'were', 'not', 'here'],\n",
       " ['you', 'are', 'a', 'loser'],\n",
       " ['i', 'love', 'indian', 'food'],\n",
       " ['who', 'is', 'down', 'for', 'a', 'restaurant'],\n",
       " ['he', 'had', 'to', 'make', 'a', 'home', 'run'],\n",
       " ['i', 'am', 'ordering', 'food'],\n",
       " ['what', 'is', 'wrong', 'with', 'you'],\n",
       " ['i', 'love', 'you'],\n",
       " ['great', 'job'],\n",
       " ['i', 'want', 'to', 'eat'],\n",
       " ['he', 'did', 'not', 'answer'],\n",
       " ['he', 'got', 'a', 'very', 'nice', 'raise'],\n",
       " ['she', 'got', 'me', 'a', 'nice', 'present'],\n",
       " ['ha', 'ha', 'ha', 'it', 'was', 'so', 'funny'],\n",
       " ['he', 'is', 'a', 'good', 'friend'],\n",
       " ['i', 'am', 'upset'],\n",
       " ['we', 'had', 'such', 'a', 'lovely', 'dinner', 'tonight'],\n",
       " ['where', 'is', 'the', 'food'],\n",
       " ['stop', 'making', 'this', 'joke', 'ha', 'ha', 'ha'],\n",
       " ['where', 'is', 'the', 'ball'],\n",
       " ['work', 'is', 'hard'],\n",
       " ['this', 'girl', 'is', 'messing', 'with', 'me'],\n",
       " ['are', 'you', 'serious'],\n",
       " ['let', 'us', 'go', 'play', 'baseball'],\n",
       " ['this', 'stupid', 'grader', 'is', 'not', 'working'],\n",
       " ['work', 'is', 'horrible'],\n",
       " ['congratulation', 'for', 'having', 'a', 'baby'],\n",
       " ['stop', 'pissing', 'me', 'off'],\n",
       " ['any', 'suggestions', 'for', 'dinner'],\n",
       " ['i', 'love', 'taking', 'breaks'],\n",
       " ['you', 'brighten', 'my', 'day'],\n",
       " ['i', 'boiled', 'rice'],\n",
       " ['she', 'is', 'a', 'bully'],\n",
       " ['why', 'are', 'you', 'feeling', 'bad'],\n",
       " ['i', 'am', 'upset'],\n",
       " ['give', 'me', 'the', 'ball'],\n",
       " ['my', 'grandmother', 'is', 'the', 'love', 'of', 'my', 'life'],\n",
       " ['enjoy', 'your', 'game'],\n",
       " ['valentine', 'day', 'is', 'near'],\n",
       " ['i', 'miss', 'you', 'so', 'much'],\n",
       " ['throw', 'the', 'ball'],\n",
       " ['my', 'life', 'is', 'so', 'boring'],\n",
       " ['she', 'said', 'yes'],\n",
       " ['will', 'you', 'be', 'my', 'valentine'],\n",
       " ['he', 'can', 'pitch', 'really', 'well'],\n",
       " ['dance', 'with', 'me'],\n",
       " ['i', 'am', 'hungry'],\n",
       " ['see', 'you', 'at', 'the', 'restaurant'],\n",
       " ['i', 'like', 'to', 'laugh'],\n",
       " ['i', 'will', 'run'],\n",
       " ['i', 'like', 'your', 'jacket'],\n",
       " ['i', 'miss', 'her'],\n",
       " ['what', 'is', 'your', 'favorite', 'baseball', 'game'],\n",
       " ['good', 'job'],\n",
       " ['i', 'love', 'you', 'to', 'the', 'stars', 'and', 'back'],\n",
       " ['what', 'you', 'did', 'was', 'awesome'],\n",
       " ['ha', 'ha', 'ha', 'lol'],\n",
       " ['i', 'do', 'not', 'want', 'to', 'joke'],\n",
       " ['go', 'away'],\n",
       " ['yesterday', 'we', 'lost', 'again'],\n",
       " ['family', 'is', 'all', 'i', 'have'],\n",
       " ['you', 'are', 'failing', 'this', 'exercise'],\n",
       " ['good', 'joke'],\n",
       " ['you', 'deserve', 'this', 'nice', 'prize'],\n",
       " ['i', 'did', 'not', 'have', 'breakfast']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs = docs_train_tokens + docs_test_tokens\n",
    "all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<PAD>', '<UNK>', 'score', 'him', 'an', 'on', 'algorithm', 'funny', 'dogs', 'proud', 'grade', 'why', 'deserve', 'feeling', 'boiled', 'smile', 'brighten', 'her', 'taking', 'life', 'incredibly', 'had', 'restaurant', 'alone', 'ball', 'code', 'forever', 'who', 'friendly', 'shouting', 'ordering', 'let', 'off', 'valentine', 'pitch', 'yesterday', 'is', 'could', 'all', 'ever', 'hungry', 'by', 'us', 'prize', 'not', 'upset', 'stadium', 'bully', 'dance', 'smiles', 'mean', 'they', 'dearest', 'your', 'raise', 'cute', 'midterm', 'cheese', 'well', 'wallet', 'assignment', 'people', 'performs', 'fun', 'cookies', 'go', 'candy', 'text', 'have', 'like', 'family', 'failing', 'points', 'for', 'worst', 'when', 'congrats', 'are', 'how', 'hate', 'horrible', 'to', 'anything', 'guy', 'favorite', 'see', 'qualified', 'dare', 'suck', 'give', 'get', 'macaroon', 'new', 'sushi', 'my', 'cake', 'wrong', 'sounds', 'we', 'indian', 'with', 'grandmother', 'me', 'mum', 'again', 'lost', 'finished', 'adore', 'few', 'away', 'in', 'loser', 'sucks', 'and', 'bravo', 'haha', 'job', 'bad', 'so', 'tasty', 'talk', 'run', 'seen', 'laugh', 'sister', 'dedication', 'project', 'humiliated', 'present', 'impressed', 'moment', 'attractive', 'plan', 'be', 'disappointed', 'date', 'ready', 'rules', 'celebrate', 'announcement', 'lol', 'come', 'boring', 'food', 'player', 'the', 'join', 'never', 'meat', 'good', 'you', 'happy', 'any', 'baby', 'work', 'looking', 'dad', 'just', 'think', 'was', 'ha', 'man', 'healthy', 'suggestions', 'stupidity', 'too', 'person', 'going', 'cooked', 'exercise', 'long', 'much', 'acceptance', 'cubs', 'takes', 'amazing', 'hard', 'french', 'dinner', 'playing', 'said', 'throw', 'where', 'what', 'dog', 'handsome', 'traction', 'rice', 'catcher', 'she', 'stop', 'he', 'great', 'play', 'kind', 'breaks', 'cutest', 'hug', 'achievements', 'miss', 'one', 'plays', 'it', 'at', 'working', 'baseball', 'chicago', 'of', 'lets', 'pizza', 'down', 'homework', 'love', 'congratulations', 'will', 'serious', 'near', 'lot', 'frustrated', 'jacket', 'saying', 'best', 'yes', 'afternoon', 'end', 'poorly', 'really', 'sad', 'this', 'puppies', 'breakfast', 'got', 'some', 'chinese', 'has', 'joke', 'day', 'after', 'such', 'can', 'hours', 'that', 'intelligent', 'eat', 'were', 'here', 'friend', 'stars', 'making', 'am', 'want', 'talented', 'stupid', 'final', 'do', 'won', 'likes', 'coming', 'zero', 'up', 'though', 'enjoy', 'limit', 'missed', 'no', 'adorable', 'messing', 'charming', 'excited', 'exam', 'failed', 'many', 'bullshit', 'did', 'game', 'together', 'vegetables', 'tonight', 'ask', 'very', 'i', 'girl', 'solve', 'lectures', 'having', 'two', 'specialization', 'a', 'brunch', 'base', 'position', 'first', 'answer', 'make', 'laughing', 'made', 'congratulation', 'pissing', 'soon', 'but', 'awful', 'waiting', 'dear', 'back', 'awesome', 'grader', 'always', 'nice', 'killing', 'home', 'approved', 'gave', 'lovely']\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "#ignore words not in glove vocabulary\n",
    "all_docs = docs_train_tokens + docs_test_tokens\n",
    "vocab = list(set([word for doc in all_docs for word in doc if word in glove_vocab]))\n",
    "vocab_ignored = set([word for doc in all_docs for word in doc if word not in glove_vocab])\n",
    "vocab.insert(0, \"<PAD>\")\n",
    "vocab.insert(1, \"<UNK>\")\n",
    "print(vocab)\n",
    "print(vocab_ignored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[147, 120, 81, 102, 104],\n",
       " [280, 249, 9, 207, 53, 198],\n",
       " [202, 36, 145, 74, 236, 110, 94, 19],\n",
       " [199, 150, 118, 171],\n",
       " [143, 36, 19],\n",
       " [280, 212, 150, 103],\n",
       " [190, 220, 272],\n",
       " [213, 5, 53, 172],\n",
       " [145, 60, 36, 165, 170],\n",
       " [280, 250, 81, 65, 193],\n",
       " [189, 273, 44, 292, 94, 67],\n",
       " [53, 164, 234, 264, 262],\n",
       " [78, 271, 72, 273, 191, 2],\n",
       " [94, 6, 62, 225],\n",
       " [280, 231, 310],\n",
       " [190, 29, 203, 102],\n",
       " [97, 69, 287, 63, 132, 160, 160],\n",
       " [264, 200, 256, 3],\n",
       " [145, 274, 157, 106],\n",
       " [280, 214, 138, 298],\n",
       " [118, 227, 150, 77, 44, 257],\n",
       " [189, 36, 94, 52, 212],\n",
       " [149, 116],\n",
       " [202, 159, 7, 140],\n",
       " [66, 36, 19],\n",
       " [145, 206, 173, 255, 104],\n",
       " [280, 249, 40],\n",
       " [280, 249, 118, 268, 81, 85, 150, 237, 118, 170],\n",
       " [150, 273, 58, 5, 150, 269],\n",
       " [208, 288, 232, 236],\n",
       " [191, 36, 118, 55],\n",
       " [78, 87, 150, 278, 241],\n",
       " [254, 150, 250, 81, 146, 102, 73, 178],\n",
       " [280, 180, 222],\n",
       " [189, 36, 131],\n",
       " [150, 88],\n",
       " [189, 49, 287, 217],\n",
       " [191, 36, 294],\n",
       " [189, 174, 26, 81, 90, 136],\n",
       " [177, 91, 36, 118, 119],\n",
       " [98, 295, 202],\n",
       " [280, 249, 268],\n",
       " [280, 107, 94, 8],\n",
       " [213],\n",
       " [228, 281, 159, 50],\n",
       " [150, 285, 77, 55],\n",
       " [94, 25, 36, 204, 299, 145, 305, 311, 102, 258],\n",
       " [228, 235, 36, 308, 102, 115],\n",
       " [254, 150, 69, 209],\n",
       " [150, 231, 287, 210, 10],\n",
       " [280, 263, 150],\n",
       " [280, 158, 280, 214, 224, 259, 23],\n",
       " [280, 231, 127, 41, 94, 124],\n",
       " [150, 77, 300],\n",
       " [280, 168, 148],\n",
       " [228, 36, 118, 7],\n",
       " [208, 169],\n",
       " [191, 36, 145, 221, 144],\n",
       " [280, 249, 167, 81, 145, 46],\n",
       " [150, 77, 20, 242, 113, 251],\n",
       " [190, 29, 203, 102],\n",
       " [27, 36, 53, 84, 144],\n",
       " [280, 69, 150, 287, 217],\n",
       " [280, 199, 3],\n",
       " [94, 184, 157, 21, 287, 108, 229],\n",
       " [280, 79, 3],\n",
       " [280, 250, 233, 143],\n",
       " [64, 77, 149],\n",
       " [17, 15, 36, 118, 267],\n",
       " [114, 73, 145, 139, 202, 231, 287, 217, 207, 186],\n",
       " [189, 201, 205],\n",
       " [191, 273, 4, 175, 116],\n",
       " [145, 153, 36, 265],\n",
       " [280, 159, 301, 73, 17, 73, 285, 240],\n",
       " [7],\n",
       " [280, 69, 202, 75, 61, 15],\n",
       " [280, 212, 8],\n",
       " [51, 77, 118, 194, 113, 28],\n",
       " [118, 117, 241, 150, 239, 44, 141, 100, 42],\n",
       " [191, 256, 205],\n",
       " [280, 249, 118, 129, 41, 53, 125, 81, 228, 126],\n",
       " [280, 249, 203, 145, 205, 274],\n",
       " [114],\n",
       " [183, 287, 63, 130],\n",
       " [280, 250, 81, 68, 93, 73, 178],\n",
       " [280, 249, 279, 134],\n",
       " [191, 239, 44, 254, 82],\n",
       " [140],\n",
       " [208, 68, 143, 275],\n",
       " [189, 36, 118, 55],\n",
       " [199, 150, 94, 302],\n",
       " [280, 249, 155, 73, 287, 135],\n",
       " [280, 249, 218],\n",
       " [280, 105, 94, 59],\n",
       " [150, 270, 145, 56],\n",
       " [160, 160, 160, 202, 159, 118, 7],\n",
       " [254, 150, 250, 81, 89, 102, 287, 197],\n",
       " [27, 36, 179, 110, 145, 253],\n",
       " [189, 36, 151],\n",
       " [150, 77, 44, 86, 73, 228, 290],\n",
       " [280, 212, 94, 156],\n",
       " [228, 83, 159, 238, 287, 235],\n",
       " [149, 235],\n",
       " [228, 286, 36, 192],\n",
       " [150, 37, 44, 282, 202],\n",
       " [280, 249, 118, 151, 73, 150],\n",
       " [76, 5, 145, 92, 116],\n",
       " [280, 249, 9, 207, 150, 26],\n",
       " [280, 250, 81, 243],\n",
       " [241, 188, 112],\n",
       " [145, 291, 289, 161, 231, 145, 24],\n",
       " [228, 36, 117],\n",
       " [150, 273, 44, 254, 53, 211],\n",
       " [280, 214, 68, 287, 57, 95],\n",
       " [254, 150, 68, 287, 24],\n",
       " [145, 283, 77, 192, 260],\n",
       " [77, 150, 210, 73, 205, 228, 223],\n",
       " [183, 77, 145, 137, 207, 145, 274],\n",
       " [280, 249, 306, 204],\n",
       " [182, 36, 145, 46],\n",
       " [189, 36, 145, 196, 166, 280, 68, 39, 122],\n",
       " [276, 77, 162],\n",
       " [191, 36, 185],\n",
       " [165, 117, 241, 150, 244, 44, 245],\n",
       " [150, 77, 287, 111],\n",
       " [280, 212, 99, 143],\n",
       " [27, 36, 210, 73, 287, 22],\n",
       " [191, 21, 81, 293, 287, 309, 121],\n",
       " [280, 249, 30, 143],\n",
       " [183, 36, 96, 100, 150],\n",
       " [280, 212, 150],\n",
       " [192, 116]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_train_ids = token2id(docs_train_tokens, vocab, unk_id=1)\n",
    "docs_train_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[280, 250, 81, 243],\n",
       " [191, 273, 44, 292],\n",
       " [191, 231, 287, 279, 307, 54],\n",
       " [189, 231, 102, 287, 307, 128],\n",
       " [160, 160, 160, 202, 159, 118, 7],\n",
       " [191, 36, 287, 149, 246],\n",
       " [280, 249, 45],\n",
       " [98, 21, 238, 287, 312, 178, 277],\n",
       " [182, 36, 145, 143],\n",
       " [190, 248, 228, 235, 160, 160, 160],\n",
       " [182, 36, 145, 24],\n",
       " [154, 36, 176],\n",
       " [228, 281, 36, 266, 100, 102],\n",
       " [77, 150, 215],\n",
       " [31, 42, 65, 193, 205],\n",
       " [228, 252, 305, 36, 44, 204],\n",
       " [154, 36, 80],\n",
       " [296, 73, 284, 287, 153],\n",
       " [190, 297, 102, 32],\n",
       " [152, 163, 73, 178],\n",
       " [280, 212, 18, 195],\n",
       " [150, 16, 94, 236],\n",
       " [280, 14, 187],\n",
       " [189, 36, 287, 47],\n",
       " [11, 77, 150, 13, 117],\n",
       " [280, 249, 45],\n",
       " [89, 102, 145, 24],\n",
       " [94, 101, 36, 145, 212, 207, 94, 19],\n",
       " [261, 53, 274],\n",
       " [33, 236, 36, 216],\n",
       " [280, 199, 150, 118, 171],\n",
       " [181, 145, 24],\n",
       " [94, 19, 36, 118, 142],\n",
       " [189, 180, 222],\n",
       " [214, 150, 133, 94, 33],\n",
       " [191, 239, 34, 226, 58],\n",
       " [48, 100, 102],\n",
       " [280, 249, 40],\n",
       " [85, 150, 203, 145, 22],\n",
       " [280, 69, 81, 123],\n",
       " [280, 214, 121],\n",
       " [280, 69, 53, 219],\n",
       " [280, 199, 17],\n",
       " [183, 36, 53, 84, 205, 274],\n",
       " [149, 116],\n",
       " [280, 212, 150, 81, 145, 247, 113, 303],\n",
       " [183, 150, 273, 159, 304],\n",
       " [160, 160, 160, 140],\n",
       " [280, 254, 44, 250, 81, 235],\n",
       " [65, 109],\n",
       " [35, 98, 105, 104],\n",
       " [70, 36, 38, 280, 68],\n",
       " [150, 77, 71, 228, 169],\n",
       " [149, 235],\n",
       " [150, 12, 228, 307, 43],\n",
       " [280, 273, 44, 68, 230]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_test_ids = token2id(docs_test_tokens, vocab, unk_id=1)\n",
    "docs_test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.83393006,  0.05308072,  0.41082636, ...,  0.40094685,\n",
       "         0.49901329, -0.23793404],\n",
       "       [-0.42028   ,  0.693     , -0.056389  , ..., -0.029277  ,\n",
       "         0.39451   , -0.17286   ],\n",
       "       ..., \n",
       "       [ 0.20393   , -0.44596   ,  0.16273   , ..., -0.34378   ,\n",
       "         0.32614   ,  0.49843   ],\n",
       "       [-0.19102   ,  0.29065   , -0.016854  , ...,  0.43286   ,\n",
       "         0.13729   ,  0.1192    ],\n",
       "       [ 0.32202   ,  0.4238    , -0.6269    , ..., -0.11275   ,\n",
       "        -0.25161   ,  0.45554   ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = build_embedding(glove_file, glove_dim, vocab)\n",
    "embeddings\n",
    "#log.info('got embedding matrix for training.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO (preprocess): save vocab, embeddings in metadata file, docs_ids in csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  147   120    81  ...      0     0     0\n",
      "  280   249     9  ...      0     0     0\n",
      "  202    36   145  ...     19     0     0\n",
      "       ...          ⋱          ...       \n",
      "  183    36    96  ...      0     0     0\n",
      "  280   212   150  ...      0     0     0\n",
      "  192   116     0  ...      0     0     0\n",
      "[torch.LongTensor of size 132x10]\n",
      "\n",
      "\n",
      "    5\n",
      "    6\n",
      "    8\n",
      "    4\n",
      "    3\n",
      "    4\n",
      "    3\n",
      "    4\n",
      "    5\n",
      "    5\n",
      "    6\n",
      "    5\n",
      "    6\n",
      "    4\n",
      "    3\n",
      "    4\n",
      "    7\n",
      "    4\n",
      "    4\n",
      "    4\n",
      "    6\n",
      "    5\n",
      "    2\n",
      "    4\n",
      "    3\n",
      "    5\n",
      "    3\n",
      "   10\n",
      "    6\n",
      "    4\n",
      "    4\n",
      "    5\n",
      "    8\n",
      "    3\n",
      "    3\n",
      "    2\n",
      "    4\n",
      "    3\n",
      "    6\n",
      "    5\n",
      "    3\n",
      "    3\n",
      "    4\n",
      "    1\n",
      "    4\n",
      "    4\n",
      "   10\n",
      "    6\n",
      "    4\n",
      "    5\n",
      "    3\n",
      "    7\n",
      "    6\n",
      "    3\n",
      "    3\n",
      "    4\n",
      "    2\n",
      "    5\n",
      "    6\n",
      "    6\n",
      "    4\n",
      "    5\n",
      "    5\n",
      "    3\n",
      "    7\n",
      "    3\n",
      "    4\n",
      "    3\n",
      "    5\n",
      "   10\n",
      "    3\n",
      "    5\n",
      "    4\n",
      "    8\n",
      "    1\n",
      "    6\n",
      "    3\n",
      "    6\n",
      "    9\n",
      "    3\n",
      "   10\n",
      "    6\n",
      "    1\n",
      "    4\n",
      "    7\n",
      "    4\n",
      "    5\n",
      "    1\n",
      "    4\n",
      "    4\n",
      "    4\n",
      "    6\n",
      "    3\n",
      "    4\n",
      "    4\n",
      "    7\n",
      "    8\n",
      "    6\n",
      "    3\n",
      "    7\n",
      "    4\n",
      "    6\n",
      "    2\n",
      "    4\n",
      "    5\n",
      "    6\n",
      "    5\n",
      "    6\n",
      "    4\n",
      "    3\n",
      "    7\n",
      "    3\n",
      "    6\n",
      "    6\n",
      "    5\n",
      "    5\n",
      "    7\n",
      "    7\n",
      "    4\n",
      "    4\n",
      "    9\n",
      "    3\n",
      "    3\n",
      "    7\n",
      "    4\n",
      "    4\n",
      "    6\n",
      "    7\n",
      "    4\n",
      "    5\n",
      "    3\n",
      "    2\n",
      "[torch.LongTensor of size 132x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train = torch.LongTensor (len(docs_train_ids), max([len(doc) for doc in docs_train_ids])).fill_(0)\n",
    "X_train_mask = torch.LongTensor (len(docs_train_ids)).fill_(0)\n",
    "for index, doc in enumerate(docs_train_ids):\n",
    "    X_train[index, :len(doc)] = torch.LongTensor(doc)\n",
    "    X_train_mask[index] = len(doc)\n",
    "    \n",
    "X_train_mask = X_train_mask.unsqueeze(1)\n",
    "\n",
    "print(X_train)\n",
    "print(X_train_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  280   250    81   243     0     0     0     0\n",
      "  191   273    44   292     0     0     0     0\n",
      "  191   231   287   279   307    54     0     0\n",
      "  189   231   102   287   307   128     0     0\n",
      "  160   160   160   202   159   118     7     0\n",
      "  191    36   287   149   246     0     0     0\n",
      "  280   249    45     0     0     0     0     0\n",
      "   98    21   238   287   312   178   277     0\n",
      "  182    36   145   143     0     0     0     0\n",
      "  190   248   228   235   160   160   160     0\n",
      "  182    36   145    24     0     0     0     0\n",
      "  154    36   176     0     0     0     0     0\n",
      "  228   281    36   266   100   102     0     0\n",
      "   77   150   215     0     0     0     0     0\n",
      "   31    42    65   193   205     0     0     0\n",
      "  228   252   305    36    44   204     0     0\n",
      "  154    36    80     0     0     0     0     0\n",
      "  296    73   284   287   153     0     0     0\n",
      "  190   297   102    32     0     0     0     0\n",
      "  152   163    73   178     0     0     0     0\n",
      "  280   212    18   195     0     0     0     0\n",
      "  150    16    94   236     0     0     0     0\n",
      "  280    14   187     0     0     0     0     0\n",
      "  189    36   287    47     0     0     0     0\n",
      "   11    77   150    13   117     0     0     0\n",
      "  280   249    45     0     0     0     0     0\n",
      "   89   102   145    24     0     0     0     0\n",
      "   94   101    36   145   212   207    94    19\n",
      "  261    53   274     0     0     0     0     0\n",
      "   33   236    36   216     0     0     0     0\n",
      "  280   199   150   118   171     0     0     0\n",
      "  181   145    24     0     0     0     0     0\n",
      "   94    19    36   118   142     0     0     0\n",
      "  189   180   222     0     0     0     0     0\n",
      "  214   150   133    94    33     0     0     0\n",
      "  191   239    34   226    58     0     0     0\n",
      "   48   100   102     0     0     0     0     0\n",
      "  280   249    40     0     0     0     0     0\n",
      "   85   150   203   145    22     0     0     0\n",
      "  280    69    81   123     0     0     0     0\n",
      "  280   214   121     0     0     0     0     0\n",
      "  280    69    53   219     0     0     0     0\n",
      "  280   199    17     0     0     0     0     0\n",
      "  183    36    53    84   205   274     0     0\n",
      "  149   116     0     0     0     0     0     0\n",
      "  280   212   150    81   145   247   113   303\n",
      "  183   150   273   159   304     0     0     0\n",
      "  160   160   160   140     0     0     0     0\n",
      "  280   254    44   250    81   235     0     0\n",
      "   65   109     0     0     0     0     0     0\n",
      "   35    98   105   104     0     0     0     0\n",
      "   70    36    38   280    68     0     0     0\n",
      "  150    77    71   228   169     0     0     0\n",
      "  149   235     0     0     0     0     0     0\n",
      "  150    12   228   307    43     0     0     0\n",
      "  280   273    44    68   230     0     0     0\n",
      "[torch.LongTensor of size 56x8]\n",
      "\n",
      "\n",
      "    4\n",
      "    4\n",
      "    6\n",
      "    6\n",
      "    7\n",
      "    5\n",
      "    3\n",
      "    7\n",
      "    4\n",
      "    7\n",
      "    4\n",
      "    3\n",
      "    6\n",
      "    3\n",
      "    5\n",
      "    6\n",
      "    3\n",
      "    5\n",
      "    4\n",
      "    4\n",
      "    4\n",
      "    4\n",
      "    3\n",
      "    4\n",
      "    5\n",
      "    3\n",
      "    4\n",
      "    8\n",
      "    3\n",
      "    4\n",
      "    5\n",
      "    3\n",
      "    5\n",
      "    3\n",
      "    5\n",
      "    5\n",
      "    3\n",
      "    3\n",
      "    5\n",
      "    4\n",
      "    3\n",
      "    4\n",
      "    3\n",
      "    6\n",
      "    2\n",
      "    8\n",
      "    5\n",
      "    4\n",
      "    6\n",
      "    2\n",
      "    4\n",
      "    5\n",
      "    5\n",
      "    2\n",
      "    5\n",
      "    5\n",
      "[torch.LongTensor of size 56x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test = torch.LongTensor (len(docs_test_ids), max([len(doc) for doc in docs_test_ids])).fill_(0)\n",
    "X_test_mask = torch.LongTensor (len(docs_test_ids)).fill_(0)\n",
    "for index, doc in enumerate(docs_test_ids):\n",
    "    X_test[index, :len(doc)] = torch.LongTensor (doc)\n",
    "    X_test_mask[index] = len(doc)\n",
    "    \n",
    "X_test_mask = X_test_mask.unsqueeze(1)\n",
    "\n",
    "print(X_test)\n",
    "print(X_test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    X_train = X_train.cuda()\n",
    "    X_train_mask = X_train_mask.cuda()\n",
    "    Y_train = Y_train.cuda()\n",
    "\n",
    "    X_test = X_test.cuda()\n",
    "    X_test_mask = X_test_mask.cuda()\n",
    "    Y_test = Y_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.cuda.LongTensor'>\n",
      "<class 'torch.cuda.LongTensor'>\n",
      "torch.Size([132, 10])\n",
      "torch.Size([132, 1])\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))\n",
    "print(type(X_train_mask))\n",
    "print(X_train.size())\n",
    "print(X_train_mask.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x209c1181898>"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = torch.cat((X_train, X_train_mask), 1)\n",
    "test2 = torch.cat((X_test, X_test_mask), 1)\n",
    "#dataset = torch.utils.data.TensorDataset(torch.cat((X_train, X_train_mask), 1), torch.cat((X_test, X_test_mask), 1))\n",
    "dataset = data_utils.TensorDataset(test1, Y_train)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "#random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "#TODO: if load model, synchronize random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word embeddings, average, \n",
    "class Model1_LR(nn.Module):\n",
    "    def __init__(self, vocab, embeddings, num_classes):\n",
    "        super(Model1_LR, self).__init__()    \n",
    "    \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        #embedding layer\n",
    "        self.embedding_dim = len(embeddings[0])\n",
    "        self.embedding = nn.Embedding(len(vocab),         #vocab size\n",
    "                                      self.embedding_dim, #embedding_dim\n",
    "                                      padding_idx=0)\n",
    "        self.embedding.weight.data = torch.Tensor(embeddings)\n",
    "        #do not backprop into embeddings\n",
    "        for p in self.embedding.parameters():\n",
    "            p.requires_grad = False\n",
    "            \n",
    "        #linear layer\n",
    "        self.linear = nn.Linear(self.embedding_dim, num_classes)\n",
    "        #nn.init.xavier_normal(self.linear.weight)\n",
    "        #self.linear.bias.data.zero_()\n",
    "     \n",
    "    def forward(self, X, X_mask):\n",
    "        #X: [m, Tx] m = batch size, Tx = word count\n",
    "        #print(X.size(), type(X))\n",
    "        m = X.size()[0]\n",
    "        Tx = X.size()[1]\n",
    "        \n",
    "        X = self.embedding(X)\n",
    "        #X: [m, Tx, embedding_dim] m = batch size, Tx = word count\n",
    "        #print(X.size(), type(X.data))\n",
    "        assert X.size() == torch.Size([m, Tx, self.embedding_dim])\n",
    "                \n",
    "        #average words in doc. use mask so we average only words not padding\n",
    "        X = torch.sum(X, 1)\n",
    "        X = Variable(torch.div(X.data, X_mask))\n",
    "        #X: [m, emb_dim]\n",
    "        #print(X.size(), type(X.data))\n",
    "        assert X.size() == torch.Size([m, self.embedding_dim])\n",
    "        \n",
    "        X = self.linear(X)\n",
    "        #X: [m, 1]\n",
    "        #print(X.size(), type(X))\n",
    "        assert X.size() == torch.Size([m, self.num_classes])\n",
    "        \n",
    "        return F.softmax(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model1_LR(vocab, embeddings, num_classes)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-2)\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model2_LSTM(nn.Module):\n",
    "    def __init__(self, vocab, embeddings, num_classes):\n",
    "        super(Model2_LSTM, self).__init__()    \n",
    "    \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        #embedding layer\n",
    "        self.embedding_dim = len(embeddings[0])\n",
    "        self.embedding = nn.Embedding(len(vocab),         #vocab size\n",
    "                                      self.embedding_dim, #embedding_dim\n",
    "                                      padding_idx=0)\n",
    "        self.embedding.weight.data = torch.Tensor(embeddings)\n",
    "        #do not backprop into embeddings\n",
    "        for p in self.embedding.parameters():\n",
    "            p.requires_grad = False\n",
    "            \n",
    "        #LSTM1, hidden_size = 128\n",
    "        #TODO: try bidirectional=True\n",
    "        self.LSTM1_hidden_size = 128\n",
    "        self.LSTM1 = nn.LSTM(self.embedding_dim, self.LSTM1_hidden_size)\n",
    "        \n",
    "        #dropout\n",
    "        self.dropout = nn.Dropout()\n",
    "     \n",
    "        #LSTM, hidden_size = 128\n",
    "        #TODO: try bidirectional=True\n",
    "        self.LSTM2_hidden_size = 128\n",
    "        self.LSTM2 = nn.LSTM(self.LSTM1_hidden_size, self.LSTM2_hidden_size)\n",
    "        \n",
    "        #linear layer\n",
    "        self.linear = nn.Linear(self.LSTM2_hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, X, X_mask):\n",
    "        #X: [m, Tx] m = batch size, Tx = word count\n",
    "        #print(X.size(), type(X))\n",
    "        m = X.size()[0]\n",
    "        Tx = X.size()[1]\n",
    "\n",
    "        #embedding layer\n",
    "        X = self.embedding(X)\n",
    "        #X: [m, Tx, embedding_dim] \n",
    "        #print(X.size(), type(X.data))\n",
    "        assert X.size() == torch.Size([m, Tx, self.embedding_dim])\n",
    "           \n",
    "        #LSTM1\n",
    "        # Transpose batch and sequence dims\n",
    "        X = X.transpose(0, 1)\n",
    "        X, _ = self.LSTM1(X)\n",
    "        # Transpose back\n",
    "        X = X.transpose(0, 1)\n",
    "        #X: [m, Tx, LSTM1_hidden_size] \n",
    "        #print(X.size(), type(X.data))\n",
    "        assert X.size() == torch.Size([m, Tx, self.LSTM1_hidden_size])\n",
    "        \n",
    "        #dropout\n",
    "        X = self.dropout(X)\n",
    "\n",
    "        #LSTM2, reduce dimension\n",
    "        # Transpose batch and sequence dims\n",
    "        X = X.transpose(0, 1)\n",
    "        _, X = self.LSTM2(X)\n",
    "        X = X[0]\n",
    "        # Transpose back\n",
    "        X = X.transpose(0, 1)\n",
    "        X = torch.squeeze(X)\n",
    "        #X: [m, LSTM2_hidden_size] \n",
    "        #print(X.size(), type(X.data))\n",
    "        assert X.size() == torch.Size([m, self.LSTM2_hidden_size])\n",
    "        \n",
    "        #dropout\n",
    "        X = self.dropout(X)\n",
    "\n",
    "        #linear\n",
    "        X = self.linear(X)\n",
    "        #X: [m, 1]\n",
    "        #print(X.size(), type(X))\n",
    "        assert X.size() == torch.Size([m, self.num_classes])\n",
    "        \n",
    "        return F.softmax(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model2_LSTM(vocab, embeddings, num_classes)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model2_LSTM (\n",
       "  (embedding): Embedding(313, 300, padding_idx=0)\n",
       "  (LSTM1): LSTM(300, 128)\n",
       "  (dropout): Dropout (p = 0.5)\n",
       "  (LSTM2): LSTM(128, 128)\n",
       "  (linear): Linear (128 -> 5)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "can't convert CUDA tensor to numpy (it doesn't support GPU arrays). Use .cpu() to move the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-74a665e2dace>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#workaround: X_train_mask to float so div works. Must be done before .cuda() call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_train_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mY_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_predict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: can't convert CUDA tensor to numpy (it doesn't support GPU arrays). Use .cpu() to move the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "#workaround: X_train_mask to float so div works. Must be done before .cuda() call\n",
    "X_train_mask = torch.FloatTensor(X_train_mask.numpy().astype(float))\n",
    "Y_predict = model(X_train, X_train_mask)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(Y_predict, Y_train)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([132, 1])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 000000 loss 1.6110 train acc 0.1364 test acc 0.1250\n",
      "epoch 000100 loss 0.9233 train acc 0.9848 test acc 0.8929\n",
      "epoch 000200 loss 0.9205 train acc 0.9848 test acc 0.9107\n",
      "epoch 000300 loss 0.9203 train acc 0.9848 test acc 0.9107\n",
      "epoch 000400 loss 0.9202 train acc 0.9848 test acc 0.9107\n",
      "epoch 000500 loss 0.9201 train acc 0.9848 test acc 0.9107\n",
      "epoch 000600 loss 0.9050 train acc 1.0000 test acc 0.8214\n",
      "epoch 000700 loss 0.9050 train acc 1.0000 test acc 0.8393\n",
      "epoch 000800 loss 0.9049 train acc 1.0000 test acc 0.8571\n",
      "epoch 000900 loss 0.9049 train acc 1.0000 test acc 0.8571\n"
     ]
    }
   ],
   "source": [
    "#TODO: use DataLoader, batches\n",
    "#move workaround just before div\n",
    "#workaround: X_train_mask to float so div works. Must be done before .cuda() call\n",
    "#X_train_mask = torch.FloatTensor(X_train_mask.numpy().astype(float))\n",
    "#X_test_mask = torch.FloatTensor(X_test_mask.numpy().astype(float))\n",
    "for epoch_local in range(1000):\n",
    "    #Forward pass\n",
    "    model.train()\n",
    "\n",
    "    Y_predict = model(X_train, X_train_mask)\n",
    "\n",
    "    #Compute loss\n",
    "    loss = criterion(Y_predict, Y_train)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        #Calculate train and test accuracy\n",
    "        _, Y_predict = torch.max(Y_predict, 1)\n",
    "        correct = (Y_predict == Y_train).sum()\n",
    "        correct = correct.cpu().data.numpy()[0]\n",
    "        accuracy_train = correct/Y_train.size(0)\n",
    "\n",
    "        model.eval()\n",
    "        Y_predict = model(X_test, X_test_mask)\n",
    "        _, Y_predict = torch.max(Y_predict, 1)\n",
    "        correct = (Y_predict == Y_test).sum()\n",
    "        correct = correct.cpu().data.numpy()[0]\n",
    "        accuracy_test = correct/Y_test.size(0)\n",
    "        \n",
    "        print(\"epoch {0:06d} loss {1:.4f} train acc {2:.4f} test acc {3:.4f}\".format(epoch, loss.cpu().data.numpy()[0], accuracy_train, accuracy_test))\n",
    "\n",
    "    #Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 4\n",
       " 3\n",
       " 0\n",
       " 0\n",
       " 2\n",
       " 0\n",
       " 3\n",
       " 4\n",
       " 4\n",
       " 2\n",
       " 1\n",
       " 3\n",
       " 3\n",
       " 3\n",
       " 1\n",
       " 3\n",
       " 3\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 0\n",
       " 2\n",
       " 4\n",
       " 3\n",
       " 3\n",
       " 3\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 3\n",
       " 2\n",
       " 0\n",
       " 1\n",
       " 2\n",
       " 4\n",
       " 4\n",
       " 2\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 2\n",
       " 0\n",
       " 2\n",
       " 2\n",
       " 0\n",
       " 1\n",
       " 2\n",
       " 0\n",
       " 3\n",
       " 2\n",
       " 2\n",
       " 4\n",
       "[torch.cuda.LongTensor of size 56 (GPU 0)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict = model(X_test, X_test_mask)\n",
    "_, Y_predict = torch.max(Y_predict, 1)\n",
    "Y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7,  0,  0,  0,  0],\n",
       "       [ 0,  8,  0,  0,  0],\n",
       "       [ 5,  0, 12,  0,  1],\n",
       "       [ 1,  1,  1, 13,  0],\n",
       "       [ 0,  0,  0,  0,  7]], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(Y_test.data.cpu().numpy(), Y_predict.data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>I love taking breaks\\t</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>My grandmother is the love of my life\\t</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>I miss you so much\\t</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>I like your jacket \\t</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>i miss her\\t</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>I love you to the stars and back\\t</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>family is all I have\\t</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          0  1\n",
       "20                   I love taking breaks\\t  0\n",
       "27  My grandmother is the love of my life\\t  0\n",
       "30                     I miss you so much\\t  0\n",
       "41                    I like your jacket \\t  0\n",
       "42                             i miss her\\t  0\n",
       "45       I love you to the stars and back\\t  0\n",
       "51                   family is all I have\\t  0"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[df_test[1] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 I love taking breaks\t\n",
      "30 I miss you so much\t\n",
      "42 i miss her\t\n",
      "51 family is all I have\t\n"
     ]
    }
   ],
   "source": [
    "for i in df_test[df_test[1] == 0].index:\n",
    "    if Y_predict[i].data.numpy()[0] == 3:\n",
    "        print(i, df_test.iloc[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
